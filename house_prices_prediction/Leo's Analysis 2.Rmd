---
title: "forward selection"
author: "Leonardo Leal Filho"
date: "4/7/2022"
output: html_document
---
```{r TheLibraries, echo=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
library(plotly)
library(GGally)
library(olsrr)
library(gridExtra)
library(lindia)
```


```{r TheDataSet, echo = FALSE}
# Reading the data
df <- read.csv("train.csv")

# This nested loop will take care of the NA values.
for(i in names(df)){
  for(j in 1:dim(df)[1]){
    if(is.na(df[j, i])){
      # Changing the NAs of the categorical features with the string "0"
      if(is.character(df[,i])){
        df[j,i] <- "0"
      }
      # Changing the NAs of the numerical features with the number 0
      else{
        df[j,i] <- 0
      }
    }
  }
}

# Making the categorical variables into factors
for(i in names(df)){
  if(is.character(df[, i])){
    df[, i] <- as.factor(df[, i])
  }
}

df <- subset(df, select = -c(Id, MSSubClass, MSZoning, Utilities, Condition2, Exterior1st,
                             Exterior2nd, MasVnrType, KitchenQual, Functional, SaleType, PoolQC))
```

# Adjusting the Levels

```{r}
# Adjusting the Levels of condition2
df$OverallCond <- factor(df$OverallCond, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

# Adjusting the Levels of BsmtQual
df$BsmtQual <- factor(df$BsmtQual, levels = c("Ex", "Gd", "TA", "Fa", "Po", "0"))


# Adjusting the Levels of BsmtCond
df$BsmtCond <- factor(df$BsmtCond, levels = c("Ex", "Gd", "TA", "Fa", "Po", "0"))


# Adjusting the Levels of PoolQC
df$PoolQC <- factor(df$PoolQC, levels = c("Ex", "Gd", "TA", "Fa", "0"))


# Making OverallCond into factor
df$OverallCond <- as.factor(df$OverallCond)

# Making OverallQual into factor
df$OverallQual <- as.factor(df$OverallQual)

```

```{r}


# Creating the full model
price_fit <- lm(log(SalePrice)~., data = df)



```



```{r ForwardSelectionModel1, echo = FALSE}

# Building the forward selection model
fwm <- ols_step_forward_p(price_fit, penter = 0.01, details = FALSE)


# Summary of the forward model
summary(fwm$model)
```


```{r ForwardSelectionModelGrid, echo = FALSE}
#Residuals QQ Plot
residuals = resid(fwm$model)
p1 = ggplot(df, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(fwm$model)


stdres2 <- rstandard(fwm$model)
#Standardized Residuals Plot
p5 = ggplot(df, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df, aes(x = seq(dffits(fwm$model)), y = dffits(fwm$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Forward Selection 1 Residual Grid"))

#Standardized Residuals vs Leverage
plot(fwm$model, which = 5, main = ("Forward Selection 1"))
```




```{r BackwardSelectionModel, echo=FALSE}

# Building the backward selection model
bwm <- ols_step_backward_p(price_fit, prem = 0.01, details = FALSE)

# Summary of the backward model
summary(bwm$model)
```



```{r BackwardSelectionModelGrid1, echo = F}
#Residuals QQ Plot
residuals = resid(bwm$model)
p1 = ggplot(df, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(bwm$model)


stdres2 <- rstandard(bwm$model)
#Standardized Residuals Plot
p5 = ggplot(df, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df, aes(x = seq(dffits(bwm$model)), y = dffits(bwm$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Backward Selection 1 Residual Grid"))

#Standardized Residuals vs Leverage
plot(bwm$model, which = 5, main = ("Backward Selection 1"))
```



```{r StepwiseSelectionModel1, echo=FALSE}


# Building the stepwise selection model
swm <- ols_step_both_p(price_fit, pent = 0.01, prem = 0.2, details = FALSE)



# Summary of the stepwise model
summary(swm$model)
```

```{r StepwiseSelectionModelGrid1, echo = F}
#Residuals QQ Plot
residuals = resid(swm$model)
p1 = ggplot(df, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(swm$model)


stdres2 <- rstandard(swm$model)
#Standardized Residuals Plot
p5 = ggplot(df, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df, aes(x = seq(dffits(swm$model)), y = dffits(swm$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Stepwise Selection 1 Residual Grid"))

#Standardized Residuals vs Leverage
plot(swm$model, which = 5, main = ("Stepwise Selection 1"))
```




## Initial Thoughts

When analyzing the models we can see that there are outliers that have a high cooks D and high leverage for all three of the models.  Because of that we will remove the two datapoints (524 and 826) and redo all three model selections.




```{r}


# Removing the outliers
df2 <- df[-c(524),]

# The new full model
price_fit2 <- lm(log(SalePrice)~., data = df2)



```



```{r ForwardSelectionModel2, echo = FALSE}


# Building the forward selection model
fwm2 <- ols_step_forward_p(price_fit2, penter = 0.01, details = FALSE)

# Summary of the forward model
summary(fwm2$model)

# The features used for the fwm2 model
fwm2Predictors <- fwm2$predictors
```



```{r ForwardSelectionModelGrid2, echo = FALSE}
#Residuals QQ Plot
residuals = resid(fwm2$model)
p1 = ggplot(df2, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df2, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(fwm2$model)

stdres2 <- rstandard(fwm2$model)
#Standardized Residuals Plot
p5 = ggplot(df2, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df2, aes(x = seq(dffits(fwm2$model)), y = dffits(fwm2$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Forward Selection 2 Residual Grid"))

#Standardized Residuals vs Leverage
plot(fwm2$model, which = 5, main = "Forward Selection 2")
```




```{r BackwardSelectionModel2, echo=FALSE}


# Building the backward selection model
bwm2 <- ols_step_backward_p(price_fit2, prem = 0.01, details = FALSE)



# Summary of the backward model
summary(bwm2$model)

# Because the backward selection gives the removed variables, to collect the variables used in the
# model I need to first create an empty vector
bwm2Predictors <- c()

# Now I add all of the features from the df2 dataframe not included in the removed list from the
# bwm2 selection model
for(i in names(df2)){
  if(!i %in% bwm2$removed){
    bwm2Predictors <- c(bwm2Predictors, i) # filling the vector with column names that was not removed
  }
}
```



```{r BackwardSelectionModelGrid2, echo = F}
#Residuals QQ Plot
residuals = resid(bwm2$model)
p1 = ggplot(df2, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df2, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(bwm2$model)


stdres2 <- rstandard(bwm2$model)
#Standardized Residuals Plot
p5 = ggplot(df2, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df2, aes(x = seq(dffits(bwm2$model)), y = dffits(bwm2$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Backward Selection 2 Residual Grid"))

#Standardized Residuals vs Leverage
plot(bwm2$model, which = 5, main = ("Backward Selection 2"))
```



```{r StepwiseSelectionModel2, echo=FALSE}


# Building the stepwise selection model
swm2 <- ols_step_both_p(price_fit2, pent = 0.01, prem = 0.2, details = FALSE)



# Summary of the stepwise model
summary(swm2$model)

```



```{r StepwiseSelectionModelGrid2, echo = F}
#Residuals QQ Plot
residuals = resid(swm2$model)
p1 = ggplot(df2, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df2, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(swm2$model)

stdres2 <- rstandard(swm2$model)
#Standardized Residuals Plot
p5 = ggplot(df2, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df2, aes(x = seq(dffits(swm2$model)), y = dffits(swm2$model))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("Stepwise Selection 2 Residual Grid"))

#Standardized Residuals vs Leverage
plot(swm2$model, which = 5, main = ("Stepwise Selection 2"))
```


## Now none of the models seem to have datapoints with a high leverage factor.  Cook's D also show that the datapoints that seem high, are actually not that high having a cook's d score of a little above 0.1.


After deleting the outliers, the forward and stepwise selection have essentially the same model, consequently the exactly same statistics.  The Backward model have a higher R-square so thus far it seems to be the best model.


```{r}
# The swm2 predictors
swm2Predictors <- swm2$predictors

# The swm2 dataframe
dfSwm <- data.frame(lSalePrice = log(df2$SalePrice))
dfSwm[, swm2Predictors] <- df2[, swm2Predictors]

swmFit <- lm(lSalePrice~OverallQual+GrLivArea+Neighborhood+BsmtFinType1+GarageCars+OverallCond+RoofMatl+
               TotalBsmtSF+YearBuilt+SaleCondition+BsmtUnfSF+MSZoning+MSSubClass+LotArea+Functional+
               CentralAir+ScreenPorch+Condition1+YearRemodAdd+Fireplaces+LandSlope+Exterior1st+BsmtExposure+
               Heating+GarageQual+KitchenQual+WoodDeckSF+OpenPorchSF+EnclosedPorch, data = dfSwm)
ols_press(swmFit)

PRESS <- function(linear.model) {
  #' calculate the predictive residuals
  pr <- round(residuals(linear.model)/(1-lm.influence(linear.model)$hat), 5)
  #' calculate the PRESS
  PRESS <- sum(pr^2)
  
  return(PRESS)
}

PRESS(swmFit)
```


```{r BackWardSelectionModelPredictors, echo = FALSE}
# Creating the predictors vector
predictors1 <- c()

# Looping through the columns of df2 to check which ones were not removed
# during the Backward Selection process
for(i in names(df2)){
  if(!i %in% bwm2$removed){
    predictors1 <- c(predictors1, i) # filling the vector with column names that was not removed
  }
}

predictors1 <- predictors1[!predictors1 %in% c("SalePrice")]
```


```{r CreatingNewModel, echo = FALSE}
# Selecting the predictors of the stepwise model selection
predictors <- swm2$predictors

# Adding the variables that will adjust the coefficient of each numerical variable with the
# Neighborhood factor
for(i in predictors){
  if(is.numeric(df2[, i])){
    predictors <- c(predictors, paste(i, "*", "Neighborhood")) # Adding the Adjustment variables
  }
}

# Creating the regression model
model12 <- lm(paste("log(SalePrice)", "~",paste(predictors, collapse =  "+")), data = df2)

# Summarizing the model
summary(model12)
```

```{r PlottingNewModelResiduals}
#Residuals QQ Plot
residuals = resid(model12)
p1 = ggplot(df2, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df2, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
p3 = gg_cooksd(model12)

stdres2 <- rstandard(model12)
#Standardized Residuals Plot
p5 = ggplot(df2, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df2, aes(x = seq(dffits(model12)), y = dffits(model12))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p5, p6, ncol = 3, top=("New Model Residual Grid"))

#Standardized Residuals vs Leverage
plot(model12, which = 5, main = ("New Model"))
```

Importing the test set and making the predictions.

```{r}
#loading the test data
test <- read.csv("test.csv")

#Selecting the predictors for the test data
test2 <- test[, swm2$predictors]
```

```{r}
# Checking which of the predictors have NA values
l <- c()
for(i in names(test2)){
  if(sum(is.na(test2[, i]))){
    l <- c(l, i)
  }
}
```

```{r}

# Taking care of the predictors NA values
for(i in l){
  for(j in 1:dim(test2)[1]){
    if(is.character(test2[,i])){
      test2[j, i] <- "0"
    }
    else{
      test2[j, i] <- 0
    }
  }
}

```

```{r}

# Transforming the character variables into factors
for(i in names(test2)){
  if(is.character(test2[, i])){
    test2[, i] <- as.factor(test2[, i])
  }
}
```

```{r}

# Making OverallCond into factor
test2$OverallCond <- as.factor(test2$OverallCond)

# Making OverallQual into factor
test2$OverallQual <- as.factor(test2$OverallQual)

```


```{r ForwardPred}

# Forward Prediction
pred <- data.frame(Id = seq(1461, 2919), SalePrice = round(exp(predict(fwm2$model, test2)), 2))

write.csv(pred, "kaggle_forward.csv", row.names = FALSE)
```


```{r StepWisePred}

# Stepward Prediction
pred <- data.frame(Id = seq(1461, 2919), SalePrice = round(exp(predict(swm2$model, test2)), 2))

write.csv(pred, "kaggle_stepwise.csv", row.names = FALSE)
```

```{r CustomPred}

# Custom Prediction
pred <- data.frame(Id = seq(1461, 2919), SalePrice = round(exp(predict(model12, test2)), 2))

write.csv(pred, "kaggle_custom.csv", row.names = FALSE)
```


```{r}
# Adapting testing dataset for the backward prediction
test3 <- test[, predictors1]

```


```{r}
# Checking which predictors have NA values
l <- c()

for(i in names(test3)){
  if(sum(is.na(test3[, i]))){
    l <- c(l, i)
  }
}
```


```{r}
# Taking care of the predictors NA values
for(i in l){
  for(j in 1:dim(test3)[1]){
    if(is.character(test3[,i])){
      test3[j, i] <- "0"
    }
    else{
      test3[j, i] <- 0
    }
  }
}
```


```{r}
# Transforming the necessary variables into factors
test3$OverallQual <- as.factor(test3$OverallQual)
test3$OverallCond <- as.factor(test3$OverallCond)
```

```{r}
# Backward Prediction
pred <- data.frame(Id = seq(1461, 2919), SalePrice = round(exp(predict(bwm2$model, test3)), 2))

write.csv(pred, "kaggle_backward.csv", row.names = FALSE)
```

